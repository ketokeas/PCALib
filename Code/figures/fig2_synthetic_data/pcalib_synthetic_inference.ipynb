{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4792bd",
   "metadata": {},
   "source": [
    "\n",
    "# Synthetic dataset + inference (pcalib): full walkthrough\n",
    "\n",
    "**What this notebook does (big picture):**\n",
    "- Builds (or loads) a synthetic *low-rank*, *time-varying* dataset using the `pcalib` library's `Potential` class.\n",
    "- Runs **two parameter sweeps** to test inference quality:\n",
    "  1. **Animals sweep:** vary the number of animals \\(D\\) while keeping trials fixed.\n",
    "  2. **Trials sweep:** vary the number of trials while keeping animals fixed.\n",
    "- For each sweep, repeatedly **simulate data** and **run inference** to estimate per-mode statistics, saving summaries to disk so the process is resumable and results are easy to aggregate later.\n",
    "\n",
    "**Key ideas:**\n",
    "- The latent signal has **K = 2** components that trace a \"corridor\" trajectory over time (a normalized path with controlled offsets).\n",
    "- We use `fit_statistics_from_dataset_diagonal(...)` to infer mode-wise statistics from synthetic recordings.\n",
    "- We extract accuracy measures via `make_predictions(...)` (specifically, `\"epsilon\"` and `\"rho\"`), and cache arrays in `cached_results/` to resume across runs.\n",
    "- Each sweep is repeated up to a cap (default: 50 attempts), enabling mean/SEM plots later.\n",
    "\n",
    "> This notebook is designed for **clarity**: each code cell has a preceding Markdown block that explains what it does (both conceptually and in detail).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a1a78",
   "metadata": {},
   "source": [
    "\n",
    "## Imports & paths\n",
    "\n",
    "**What this cell does:**\n",
    "- Imports standard libraries and the required `pcalib` functions/classes.\n",
    "- Creates (if necessary) a `cached_results/` directory to store all outputs.\n",
    "- Optionally sets a NumPy random seed so data generation is reproducible across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7e4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pcalib.functions import fit_statistics_from_dataset_diagonal, make_predictions\n",
    "from pcalib.classes import Potential\n",
    "from pcalib.utils import PCA_matlab_like, generate_gaussian_correlation_matrix\n",
    "\n",
    "# Results directory (consistent spelling)\n",
    "OUT = Path.cwd() / \"cached_results\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional reproducibility\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09185b",
   "metadata": {},
   "source": [
    "\n",
    "## Helper functions: resume/cap logic and scalar coercion\n",
    "\n",
    "**What this cell does:**\n",
    "- Defines small utilities used throughout the notebook:\n",
    "  - `attempts_done(path)`: returns how many attempts (rows) are already saved in a given `.npy` file.\n",
    "  - `append_rows_capped(path, new_block, cap)`: appends attempts along axis 0 but **never exceeds** `cap` total attempts in the saved file.\n",
    "  - `_to_scalar(x)`: converts array-like values (incl. 0D/1D/2D NumPy/JAX) to plain Python floats for clean storage/logging.\n",
    "\n",
    "**Details & rationale:**\n",
    "- The sweeps are designed to be **resumable**. If you've already run 17 attempts out of 50, re-running the \"attempts\" cells will only append up to the cap.\n",
    "- `_to_scalar` avoids shape/dtype surprises when moving values into NumPy arrays and then onto disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfdcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attempts_done(path: Path):\n",
    "    return np.load(path).shape[0] if path.exists() else 0\n",
    "\n",
    "def append_rows_capped(path: Path, new_block, cap: int):\n",
    "    \"\"\"\n",
    "    Append new rows on axis 0, but ensure the saved file has <= cap rows total.\n",
    "    new_block must have shape [num_new, ...].\n",
    "    \"\"\"\n",
    "    if new_block is None or len(new_block) == 0:\n",
    "        return\n",
    "    if path.exists():\n",
    "        old = np.load(path)\n",
    "        need = max(0, cap - old.shape[0])\n",
    "        if need == 0:\n",
    "            return  # already at cap\n",
    "        out = np.concatenate([old, new_block[:need]], axis=0)\n",
    "    else:\n",
    "        out = new_block[:cap]\n",
    "\n",
    "    np.save(path, out)  # overwrite existing file directly\n",
    "\n",
    "def _to_scalar(x):\n",
    "    a = np.asarray(x)\n",
    "    if a.ndim == 0:\n",
    "        return float(a)\n",
    "    if a.ndim == 1:\n",
    "        return float(a[0])\n",
    "    if a.ndim == 2:\n",
    "        return float(a[0, 0])\n",
    "    return float(a.reshape(-1)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301e940",
   "metadata": {},
   "source": [
    "\n",
    "## Latent signal generator: the 2D \"corridor\"\n",
    "\n",
    "**What this cell does (conceptually):**\n",
    "- Builds a **T×2** latent trajectory (`K=2`) that moves through a corridor-like path:\n",
    "  - Linear drift in the first quarter (component 1).\n",
    "  - A circular arc (via cos/sin) in the middle half (components 1 & 2).\n",
    "  - Linear drift back in the last quarter.\n",
    "  - Anti-symmetric vertical offsets (−ε for first half, +ε for second half) on component 2.\n",
    "\n",
    "**What this cell does (details):**\n",
    "- After constructing the piecewise path, we zero-mean each component, **variance-normalize**, then rescale to a target variance `var_array`.\n",
    "- This becomes the **ground-truth** latent path `\\bar{x}_t` used in the generative model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29d8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corridor_signal(T, var_array, epsilon_corridor):\n",
    "    signal_array = np.zeros([T, 2])  # two components: K=2\n",
    "    # Break everything into quarters\n",
    "    signal_array[:T//4, 0] = np.linspace(-2, 0, T//4)\n",
    "\n",
    "    n_points_middle = np.shape(np.arange(T//4, (3*T)//4))[0]\n",
    "    signal_array[T//4:(3*T)//4, 0] = -np.cos(2*np.pi*np.arange(n_points_middle)/n_points_middle) + 1\n",
    "    signal_array[T//4:(3*T)//4, 1] = -np.sin(2*np.pi*np.arange(n_points_middle)/n_points_middle)\n",
    "\n",
    "    n_points_end = np.shape(signal_array[(3*T)//4:, 0])[0]\n",
    "    signal_array[(3*T)//4:, 0] = np.linspace(0, -2, n_points_end)\n",
    "    signal_array[:T//2, 1] -= epsilon_corridor\n",
    "    signal_array[T//2:, 1] += epsilon_corridor\n",
    "\n",
    "    signal_array -= np.mean(signal_array, 0, keepdims=True)\n",
    "    signal_array /= np.sqrt(np.var(signal_array, 0, keepdims=True))\n",
    "    signal_array[:, 0] *= np.sqrt(var_array[0])\n",
    "    signal_array[:, 1] *= np.sqrt(var_array[1])\n",
    "\n",
    "    return signal_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f9d0c",
   "metadata": {},
   "source": [
    "\n",
    "## Global configuration & sweep grids\n",
    "\n",
    "**What this cell does:**\n",
    "- Centralizes all key parameters so you can change them in one place.\n",
    "- Defines the **sweep grids**: `D_array` (number of animals) and `n_trials_array` (number of trials) used later.\n",
    "- Sets the **cap** (`n_attempts`) on how many times each sweep is repeated (for bootstrapped stability).\n",
    "\n",
    "**Notes:**\n",
    "- `load_potential` controls whether we **generate** ground-truth and save it, or **load** previously saved objects. The default below is `False` so the notebook is self-contained on first run.\n",
    "- If you've already generated and saved the Potentials once, set `load_potential=True` to skip regeneration and use the saved `.npz` bundles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Toggle: generate truth vs. load existing\n",
    "load_potential = False   # set True if you've already generated/saved once and want to reload\n",
    "\n",
    "# Baseline sizes (when generating)\n",
    "T = 100\n",
    "N_per_animal = 50\n",
    "K = 2\n",
    "D = 2\n",
    "n_trials = 40\n",
    "\n",
    "# Sweep grids\n",
    "n_trials_array = np.arange(5, 50, 5)   # 5,10,...,45\n",
    "D_array = np.arange(1, 6)              # 1..5\n",
    "\n",
    "# Temporal correlation scales\n",
    "tau_sigma = 2                          # within-trial smoothing kernel width\n",
    "tau_xi   = 5                           # trial-to-trial temporal correlation scale\n",
    "\n",
    "# Latent signal amplitude targets\n",
    "var_array = [2, 1]\n",
    "epsilon_corridor = 0.1\n",
    "\n",
    "# Attempts cap (per sweep)\n",
    "n_attempts = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872169c",
   "metadata": {},
   "source": [
    "\n",
    "## Build or load the generative `Potential`s\n",
    "\n",
    "**What this cell does (conceptually):**\n",
    "- Either **generates** and **saves** two `Potential` objects or **loads** them from disk:\n",
    "  - **Many-animals Potential**: sized to the *largest* `D` in the sweep (so we can subselect animals from the largest `D`).\n",
    "  - **Many-trials Potential**: sized to the baseline `D` (used for the trials sweep).\n",
    "\n",
    "**What this cell does (details when generating):**\n",
    "- Construct the ground-truth latent `bar_x = corridor_signal(...)`.\n",
    "- Create a random orthonormal **mode basis** `\\bar{e}` (QR-orthogonalized) and scale columns to have norm \\(\\sqrt{N}\\).\n",
    "- Define temporal correlation kernels:\n",
    "  - `Z` (within-trial smoothing) via a Gaussian correlation matrix with scale `tau_sigma * sqrt(2)` - this is a correlation matrix of a pure white noise smoothed with a Gaussian kernel of with `tau_sigma`.\n",
    "  - `Delta` (trial-to-trial temporal correlation) with scale `tau_xi`.\n",
    "- Draw per-neuron noise scales `\\bar{\\sigma}`.\n",
    "- Build per-animal selector blocks `G` so each animal's neurons form identity blocks along the diagonal.\n",
    "- Set `Xi = 0` to disable neuron-dependent kernels (as per the paper's assumptions).\n",
    "- Save:\n",
    "  - The two `Potential` bundles (`many_animals_potential.npz`, `many_trials_potential.npz`),\n",
    "  - Reference arrays (`D_array`, `n_trials_array`, and their baselines),\n",
    "  - `tau_sigma` and some truth summaries for later comparison.\n",
    "\n",
    "**Why this separation matters:**\n",
    "- The animals sweep needs the *largest* version so we can subselect animals cleanly.\n",
    "- The trials sweep needs a fixed-`D` version so we can subselect trials cleanly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b68f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not load_potential:\n",
    "    # Derived sizes\n",
    "    N = N_per_animal * D\n",
    "\n",
    "    # Ground-truth latent\n",
    "    bar_x = corridor_signal(T, var_array, epsilon_corridor)\n",
    "\n",
    "    # Mode basis up to max D\n",
    "    bar_e_largest = np.random.normal(0, 1, [N_per_animal * D_array[-1], K])\n",
    "    bar_e_largest, _ = np.linalg.qr(bar_e_largest)  # orthogonalize\n",
    "    bar_e_largest *= np.sqrt(N_per_animal * D_array[-1])  # scale columns\n",
    "    bar_e = bar_e_largest[:N, :]\n",
    "\n",
    "    # Temporal noise kernel (within-trial)\n",
    "    Z = generate_gaussian_correlation_matrix(T, tau_sigma * np.sqrt(2))\n",
    "\n",
    "    # Per-neuron noise scales up to max D\n",
    "    bar_sigma_largest = np.abs(np.random.normal(1, 0.1, N_per_animal * D_array[-1]))\n",
    "    bar_sigma = bar_sigma_largest[:N]\n",
    "\n",
    "    # Trial-to-trial structure\n",
    "    Delta = generate_gaussian_correlation_matrix(T, tau_xi)\n",
    "    bar_xi_largest = np.zeros([D_array[-1], K])\n",
    "    for k in range(K):\n",
    "        bar_xi_largest[:, k] = np.sqrt(np.abs(np.random.normal(2/(k+1), 0.1/(k+1), D_array[-1])))\n",
    "\n",
    "    G_largest = np.zeros([D_array[-1], N_per_animal * D_array[-1], N_per_animal * D_array[-1]])\n",
    "    for d in range(D_array[-1]):\n",
    "        sl = slice(d*N_per_animal, (d+1)*N_per_animal)\n",
    "        G_largest[d, sl, sl] = np.eye(N_per_animal)\n",
    "\n",
    "    bar_xi = bar_xi_largest[:D, :]\n",
    "    G = G_largest[:D, :N, :N]\n",
    "\n",
    "    # Neuron-dependent kernel disabled\n",
    "    Xi = np.zeros([T, T])\n",
    "\n",
    "    # Save truth summaries\n",
    "    np.save(OUT / \"true_mean_noise_variance.npy\", np.sqrt(np.mean(bar_sigma**2 / n_trials)))\n",
    "    np.save(OUT / \"true_signal_variability.npy\", np.var(bar_x, 0))\n",
    "\n",
    "    # Save the two Potentials\n",
    "    many_animals_potential = Potential(bar_sigma_largest, bar_e_largest, G_largest, bar_xi_largest, Z, Delta, bar_x, Xi)\n",
    "    many_animals_potential.save_as_npz(str(OUT / \"many_animals_potential.npz\"))\n",
    "\n",
    "    many_trials_potential = Potential(bar_sigma, bar_e, G, bar_xi, Z, Delta, bar_x, Xi)\n",
    "    many_trials_potential.save_as_npz(str(OUT / \"many_trials_potential.npz\"))\n",
    "\n",
    "    # Save references\n",
    "    np.save(OUT / \"D_array.npy\", D_array)\n",
    "    np.save(OUT / \"D_reference.npy\", D)\n",
    "    np.save(OUT / \"n_trials_array.npy\", n_trials_array)\n",
    "    np.save(OUT / \"n_trials_reference.npy\", n_trials)\n",
    "    np.save(OUT / \"tau_sigma.npy\", tau_sigma)\n",
    "else:\n",
    "    # Load the saved bundles and references\n",
    "    many_animals_potential = Potential.from_npz(str(OUT / \"many_animals_potential.npz\"))\n",
    "    many_trials_potential = Potential.from_npz(str(OUT / \"many_trials_potential.npz\"))\n",
    "    D_array = np.load(OUT / \"D_array.npy\")\n",
    "    n_trials_array = np.load(OUT / \"n_trials_array.npy\")\n",
    "    D = int(np.load(OUT / \"D_reference.npy\"))\n",
    "    n_trials = int(np.load(OUT / \"n_trials_reference.npy\"))\n",
    "    tau_sigma = np.load(OUT / \"tau_sigma.npy\").item()\n",
    "\n",
    "    # Pull sizes/components from loaded objects\n",
    "    bar_e_largest = many_animals_potential.bar_e\n",
    "    bar_e = many_trials_potential.bar_e\n",
    "    T, K = np.shape(many_animals_potential.bar_x)\n",
    "    N = np.shape(many_trials_potential.bar_sigma)[0]\n",
    "    G_largest = many_animals_potential.G\n",
    "    bar_xi_largest = many_animals_potential.bar_xi\n",
    "    N_per_animal = N // D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3bb9d",
   "metadata": {},
   "source": [
    "\n",
    "## Quick visualization: ground-truth latent trajectories (optional)\n",
    "\n",
    "**What this cell does:**\n",
    "- Plots the two latent components `\\bar{x}_t` over time.\n",
    "- Useful to verify the qualitative shape of the \"corridor\" signal (two components with different scalings and offsets).\n",
    "\n",
    "**Details:**\n",
    "- If you just generated the potentials, both `many_animals_potential` and `many_trials_potential` share the same `bar_x`, so plotting either is fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(many_animals_potential.bar_x)\n",
    "plt.title(\"Ground-truth latent trajectories (K=2)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d940c",
   "metadata": {},
   "source": [
    "\n",
    "## Animals sweep — setup (resume-aware)\n",
    "\n",
    "**What this cell does:**\n",
    "- Declares file paths for the results of the **animals sweep** (varying `D` with fixed `n_trials`).\n",
    "- Computes how many **attempts** have already been performed (by reading the number of rows in each `.npy` file).\n",
    "- Derives how many attempts remain up to the cap `n_attempts`.\n",
    "\n",
    "**Files produced (shapes across attempts):**\n",
    "- `epsilon_animals.npy`: shape `(attempts, |D_array|, K)`\n",
    "- `rho_animals.npy`: shape `(attempts, |D_array|, K)`\n",
    "- `signal_variability_animals.npy`: shape `(attempts, |D_array|, K)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc71506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "animals_files = {\n",
    "    \"epsilon\": OUT / \"epsilon_animals.npy\",\n",
    "    \"rho\": OUT / \"rho_animals.npy\",\n",
    "    \"sigvar\": OUT / \"signal_variability_animals.npy\",\n",
    "}\n",
    "done_animals = max(\n",
    "    attempts_done(animals_files[\"epsilon\"]),\n",
    "    attempts_done(animals_files[\"rho\"]),\n",
    "    attempts_done(animals_files[\"sigvar\"]),\n",
    ")\n",
    "remaining_animals = max(0, n_attempts - done_animals)\n",
    "remaining_animals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5560bde",
   "metadata": {},
   "source": [
    "\n",
    "## Animals sweep — run attempts (outer loop over attempts, inner loop over `D_array`)\n",
    "\n",
    "**What this cell does (conceptually):**\n",
    "- For each *remaining* attempt up to the cap:\n",
    "  1. **Simulate** a dataset with `n_trials` trials from the **many-animals** Potential.\n",
    "  2. For each `D_current` in `D_array`:\n",
    "     - **Subselect neurons** so we keep only the first `N_per_animal * D_current` neurons (i.e., that many animals).\n",
    "     - **Compute PCA** on the trial-averaged data to get eigenvectors/scores, then **sign-align** the first `K` components with the true `\\bar{e}` (so plots/predictions are consistent across attempts).\n",
    "     - On the **very first** (global) attempt at the **baseline** `D`, save the PCA scores to `inferred_y_{n_trials}_trials.npy` (handy for visualization).\n",
    "     - **Run inference** via `fit_statistics_from_dataset_diagonal(current_data, K, current_G, tau_sigma, gamma=0.1)` to estimate per-mode parameters.\n",
    "     - **Make predictions** for each mode using `make_predictions(...)` and extract `\"epsilon\"` and `\"rho\"`, coercing to float with `_to_scalar(...)`.\n",
    "     - **Record** the inferred signal variance per mode as a diagnostic (variance of inferred `bar_x` along time).\n",
    "  3. **Append** this attempt's results to the `.npy` files using `append_rows_capped(...)`.\n",
    "\n",
    "**Why PCA and sign-alignment?**\n",
    "- PCA eigenvectors are only defined up to sign. To ensure consistent orientation with the ground truth (and across attempts), we align signs using the dot product with the true `\\bar{e}` columns (normalized to \\(\\sqrt{N}\\)).\n",
    "\n",
    "**Loop structure:**\n",
    "- **Outer**: attempts (resume-aware, up to `n_attempts` rows).\n",
    "- **Inner**: `D_current` in `D_array` (e.g., 1..5 animals). For each, run PCA, inference, predictive summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if remaining_animals > 0:\n",
    "    for attempt in range(remaining_animals):\n",
    "        print(f\"[animals] attempt {done_animals + attempt + 1} of {n_attempts}\")\n",
    "        synth_data_large = many_animals_potential.generate_sample_data(n_samples=n_trials)\n",
    "\n",
    "        epsilon_animals_new = np.zeros([np.shape(D_array)[0], K])\n",
    "        rho_animals_new = np.zeros([np.shape(D_array)[0], K])\n",
    "        signal_variability_new = np.zeros([np.shape(D_array)[0], K])\n",
    "\n",
    "        for i, D_current in enumerate(D_array):\n",
    "            # Subselect to current number of animals\n",
    "            current_N = N_per_animal * D_current\n",
    "            current_data = synth_data_large[:, :, :current_N]\n",
    "            current_bar_e = np.array(bar_e_largest[:current_N, :])  # ensure mutability\n",
    "            current_G = G_largest[:D_current, :current_N, :current_N]\n",
    "\n",
    "            # PCA on trial-averaged data (shape T x current_N)\n",
    "            coeff, score, _ = PCA_matlab_like(np.mean(current_data, 0))\n",
    "            sign_array = np.zeros(K)\n",
    "            for k in range(K):\n",
    "                # Normalize true vector for fair dot product, target norm sqrt(N)\n",
    "                current_bar_e[:, k] = current_bar_e[:, k] / np.linalg.norm(current_bar_e[:, k]) * np.sqrt(current_N)\n",
    "                sign_array[k] = np.sign(np.dot(coeff[:, k], current_bar_e[:, k]))\n",
    "\n",
    "            # Rescale first K components to match sqrt(N) convention and apply signs\n",
    "            coeff = coeff[:, :K] * sign_array[np.newaxis, :] * np.sqrt(current_N)\n",
    "            score = score[:, :K] * sign_array[np.newaxis, :] / np.sqrt(current_N)\n",
    "\n",
    "            # Save PCA scores once at baseline D on the very first global attempt\n",
    "            if done_animals + attempt == 0 and D_current == D:\n",
    "                np.save(OUT / f\"inferred_y_{n_trials}_trials.npy\", score)\n",
    "\n",
    "            # Inference per mode (diagonal approx)\n",
    "            inferred_potentials, _ = fit_statistics_from_dataset_diagonal(current_data, K, current_G, tau_sigma, gamma=0.1)\n",
    "\n",
    "            # Predictions + diagnostics\n",
    "            for k in range(K):\n",
    "                prediction_dict = make_predictions(inferred_potentials[k])\n",
    "                epsilon_animals_new[i, k] = _to_scalar(prediction_dict[\"epsilon\"])\n",
    "                rho_animals_new[i, k]     = _to_scalar(prediction_dict[\"rho\"])\n",
    "                signal_variability_new[i, k] = np.var(inferred_potentials[k].bar_x, 0)[0]\n",
    "\n",
    "        # Append one attempt (row) to each file, capped\n",
    "        append_rows_capped(animals_files[\"epsilon\"], epsilon_animals_new[np.newaxis, :, :], n_attempts)\n",
    "        append_rows_capped(animals_files[\"rho\"], rho_animals_new[np.newaxis, :, :], n_attempts)\n",
    "        append_rows_capped(animals_files[\"sigvar\"], signal_variability_new[np.newaxis, :, :], n_attempts)\n",
    "else:\n",
    "    print(f\"[animals] already at cap ({n_attempts}) attempts; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc68dd0",
   "metadata": {},
   "source": [
    "\n",
    "## Trials sweep — setup (resume-aware)\n",
    "\n",
    "**What this cell does:**\n",
    "- Declares file paths for the results of the **trials sweep** (varying the number of trials while keeping animals fixed).\n",
    "- Computes existing attempts and remaining attempts up to the cap.\n",
    "\n",
    "**Files produced (shapes across attempts):**\n",
    "- `epsilon_trials.npy`: shape `(attempts, |n_trials_array|, K)`\n",
    "- `rho_trials.npy`: shape `(attempts, |n_trials_array|, K)`\n",
    "- `sqrt_mean_sigma_squared.npy`: shape `(attempts, |n_trials_array|)` — a global noise summary `\\(\\sqrt{\\mathbb{E}_i[\\bar{\\sigma}_i^2]}\\)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c04701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trials_files = {\n",
    "    \"epsilon\": OUT / \"epsilon_trials.npy\",\n",
    "    \"rho\": OUT / \"rho_trials.npy\",\n",
    "    \"sigma\": OUT / \"sqrt_mean_sigma_squared.npy\",\n",
    "}\n",
    "done_trials = max(\n",
    "    attempts_done(trials_files[\"epsilon\"]),\n",
    "    attempts_done(trials_files[\"rho\"]),\n",
    "    attempts_done(trials_files[\"sigma\"]),\n",
    ")\n",
    "remaining_trials = max(0, n_attempts - done_trials)\n",
    "remaining_trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2a82e",
   "metadata": {},
   "source": [
    "\n",
    "## Trials sweep — run attempts (outer loop over attempts, inner loop over `n_trials_array`)\n",
    "\n",
    "**What this cell does (conceptually):**\n",
    "- For each *remaining* attempt up to the cap:\n",
    "  1. **Simulate** a dataset with the **maximum** number of trials from the **many-trials** Potential.\n",
    "  2. For each `n_trials_current` in `n_trials_array`:\n",
    "     - **Subselect trials**: keep the first `n_trials_current` trials.\n",
    "     - **Compute PCA** on the trial-averaged data and **sign-align** to the baseline `\\bar{e}`.\n",
    "     - **Run inference** via `fit_statistics_from_dataset_diagonal(current_data, K, G, tau_sigma, gamma=0.1)`.\n",
    "     - **Make predictions** for each mode (extract `\"epsilon\"` and `\"rho\"`).\n",
    "     - **Compute a noise summary**: \\(\\sqrt{\\mathbb{E}_i[\\bar{\\sigma}_i^2]}\\) from the inferred Potential (mode 0 is used as representative).\n",
    "  3. **Append** this attempt's results to disk, respecting the overall cap.\n",
    "\n",
    "**Loop structure:**\n",
    "- **Outer**: attempts (resume-aware).\n",
    "- **Inner**: `n_trials_current` in `n_trials_array` (e.g., 5..45). For each, run PCA, inference, predictive summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bf6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if remaining_trials > 0:\n",
    "    G = many_trials_potential.G\n",
    "    N_const = many_trials_potential.bar_sigma.shape[0]\n",
    "    for attempt in range(remaining_trials):\n",
    "        print(f\"[trials] attempt {done_trials + attempt + 1} of {n_attempts}\")\n",
    "        synth_data_large = many_trials_potential.generate_sample_data(n_samples=n_trials_array[-1])\n",
    "\n",
    "        epsilon_trials_new = np.zeros([np.shape(n_trials_array)[0], K])\n",
    "        rho_trials_new = np.zeros([np.shape(n_trials_array)[0], K])\n",
    "        sqrt_mean_sigma_squared_new = np.zeros([np.shape(n_trials_array)[0]])\n",
    "\n",
    "        for i, n_trials_current in enumerate(n_trials_array):\n",
    "            current_data = synth_data_large[:n_trials_current, :, :]\n",
    "\n",
    "            # PCA on trial-averaged data (shape T x N)\n",
    "            coeff, score, _ = PCA_matlab_like(np.mean(current_data, 0))\n",
    "            sign_array = np.zeros(K)\n",
    "            for k in range(K):\n",
    "                sign_array[k] = np.sign(np.dot(coeff[:, k], many_trials_potential.bar_e[:, k]))\n",
    "\n",
    "            # Rescale first K components to match sqrt(N) convention and apply signs\n",
    "            coeff = coeff[:, :K] * sign_array[np.newaxis, :] * np.sqrt(N_const)\n",
    "            score = score[:, :K] * sign_array[np.newaxis, :] / np.sqrt(N_const)\n",
    "\n",
    "            # Inference\n",
    "            inferred_potentials, _ = fit_statistics_from_dataset_diagonal(current_data, K, G, tau_sigma, gamma=0.1)\n",
    "\n",
    "            # Aggregate noise summary (representative from mode 0)\n",
    "            sqrt_mean_sigma_squared_new[i] = np.sqrt(np.mean(inferred_potentials[0].bar_sigma**2))\n",
    "\n",
    "            # Predictions per mode\n",
    "            for k in range(K):\n",
    "                prediction_dict = make_predictions(inferred_potentials[k])\n",
    "                epsilon_trials_new[i, k] = _to_scalar(prediction_dict[\"epsilon\"])\n",
    "                rho_trials_new[i, k]     = _to_scalar(prediction_dict[\"rho\"])\n",
    "\n",
    "        # Append one attempt (row) to each file, capped\n",
    "        append_rows_capped(trials_files[\"epsilon\"], epsilon_trials_new[np.newaxis, :, :], n_attempts)\n",
    "        append_rows_capped(trials_files[\"rho\"], rho_trials_new[np.newaxis, :, :], n_attempts)\n",
    "        append_rows_capped(trials_files[\"sigma\"], sqrt_mean_sigma_squared_new[np.newaxis, :], n_attempts)\n",
    "else:\n",
    "    print(f\"[trials] already at cap ({n_attempts}) attempts; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f049d2",
   "metadata": {},
   "source": [
    "\n",
    "## Load cached arrays for analysis\n",
    "\n",
    "**What this cell does:**\n",
    "- Provides a small helper to load a `.npy` array if it exists, returning `None` otherwise.\n",
    "- Loads all animals-sweep and trials-sweep arrays for downstream summarization and plotting.\n",
    "\n",
    "**Why this matters:**\n",
    "- After running attempts, you'll typically want to **aggregate** results across attempts (e.g., compute mean and SEM per grid point).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_attempts(path): \n",
    "    return np.load(path) if Path(path).exists() else None\n",
    "\n",
    "eps_anim = load_attempts(OUT / \"epsilon_animals.npy\")\n",
    "rho_anim = load_attempts(OUT / \"rho_animals.npy\")\n",
    "sigvar_anim = load_attempts(OUT / \"signal_variability_animals.npy\")\n",
    "\n",
    "eps_trials = load_attempts(OUT / \"epsilon_trials.npy\")\n",
    "rho_trials = load_attempts(OUT / \"rho_trials.npy\")\n",
    "sigma_trials = load_attempts(OUT / \"sqrt_mean_sigma_squared.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce7b6e7",
   "metadata": {},
   "source": [
    "\n",
    "## Mean/SEM helper\n",
    "\n",
    "**What this cell does:**\n",
    "- Defines a helper function `mean_sem(x, axis=0)` that returns the mean and standard error of the mean (SEM) along the specified axis.\n",
    "- We'll use this to aggregate across **attempts** (axis 0) to visualize how estimates evolve with `D` or with the number of trials.\n",
    "\n",
    "**Details:**\n",
    "- SEM = standard deviation / sqrt(n).\n",
    "- We use `ddof=1` to get an unbiased standard deviation estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_sem(x, axis=0):\n",
    "    m = x.mean(axis=axis)\n",
    "    s = x.std(axis=axis, ddof=1) / np.sqrt(x.shape[axis])\n",
    "    return m, s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4e465",
   "metadata": {},
   "source": [
    "\n",
    "## Example plots: inspecting sweep results (optional)\n",
    "\n",
    "**What this cell does:**\n",
    "- Produces a few example plots to visualize the dependence of predictions on the sweep variable:\n",
    "  - `epsilon` (mode 1) vs **# animals**.\n",
    "  - `rho` (mode 1) vs **# trials**.\n",
    "- You can duplicate/adapt these blocks for mode 2 (index `1`) and for other metrics.\n",
    "\n",
    "**Notes:**\n",
    "- We guard plots with `if arr is not None` to avoid errors if you haven't run the sweeps yet.\n",
    "- The arrays have the shape `(attempts, grid, K)`, so we take `mean_sem(..., axis=0)` to aggregate across attempts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Epsilon (mode 1) vs number of animals\n",
    "if eps_anim is not None:\n",
    "    m, s = mean_sem(eps_anim, axis=0)  # shape: |D_array| x K\n",
    "    plt.figure()\n",
    "    plt.errorbar(D_array, m[:,0], yerr=s[:,0])\n",
    "    plt.title(\"Epsilon (mode 1) vs # animals\")\n",
    "    plt.xlabel(\"# animals (D)\")\n",
    "    plt.ylabel(\"epsilon\")\n",
    "    plt.show()\n",
    "\n",
    "# Rho (mode 1) vs number of trials\n",
    "if rho_trials is not None:\n",
    "    m, s = mean_sem(rho_trials, axis=0)  # shape: |n_trials_array| x K\n",
    "    plt.figure()\n",
    "    plt.errorbar(n_trials_array, m[:,0], yerr=s[:,0])\n",
    "    plt.title(\"Rho (mode 1) vs # trials\")\n",
    "    plt.xlabel(\"# trials\")\n",
    "    plt.ylabel(\"rho\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9e4c5",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & next steps\n",
    "\n",
    "- **Resetting the cache:** To start over, delete the contents of the `cached_results/` folder.\n",
    "- **Changing the model:** Try different `tau_sigma`, `tau_xi`, or `var_array` to see how inference behaves under different temporal/noise conditions.\n",
    "- **Adding widgets:** For interactive exploration, add sliders (e.g., `ipywidgets`) for parameters like `n_attempts`, `D_array`, `n_trials_array` and re-run the sweep cells.\n",
    "- **Parallelizing attempts:** If attempts are slow for your dataset size, consider distributing attempts across processes or machines; the on-disk append makes combining results straightforward.\n",
    "- **Reproducibility:** Keep the `np.random.seed(...)` in the imports cell to ensure repeated runs produce the same synthetic data and results (useful for debugging changes).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
